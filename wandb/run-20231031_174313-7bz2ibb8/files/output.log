You are adding a <class 'transformers.trainer_callback.DefaultFlowCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
wandb: WARNING Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'learning_rate' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
  0%|          | 0/7850 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.






















  2%|▏         | 157/7850 [00:50<37:34,  3.41it/s]
  0%|          | 0/68 [00:00<?, ?it/s]






















 96%|█████████▌| 65/68 [00:41<00:01,  1.53it/s]























  4%|▍         | 315/7850 [02:37<36:34,  3.43it/s]
  0%|          | 0/68 [00:00<?, ?it/s]





















100%|██████████| 68/68 [00:42<00:00,  1.65it/s]
























  6%|▌         | 470/7850 [04:20<35:52,  3.43it/s]
  6%|▌         | 472/7850 [04:20<36:16,  3.39it/s]






















 99%|█████████▊| 67/68 [00:44<00:00,  1.45it/s]

























  8%|▊         | 630/7850 [06:08<34:59,  3.44it/s]
{'loss': 0.1721, 'learning_rate': 0.00012005201603878226, 'epoch': 4.0}





















 97%|█████████▋| 66/68 [00:43<00:01,  1.52it/s]
























 10%|█         | 786/7850 [07:56<34:39,  3.40it/s]
 10%|█         | 787/7850 [07:56<34:34,  3.40it/s]






















 96%|█████████▌| 65/68 [00:42<00:01,  1.51it/s]























 12%|█▏        | 942/7850 [09:40<33:49,  3.40it/s]
 12%|█▏        | 945/7850 [09:41<33:28,  3.44it/s]

















 99%|█████████▊| 67/68 [00:34<00:00,  1.92it/s]
























 14%|█▍        | 1100/7850 [11:16<32:39,  3.44it/s]
 14%|█▍        | 1102/7850 [11:17<32:34,  3.45it/s]


















 97%|█████████▋| 66/68 [00:36<00:01,  1.78it/s]
























 16%|█▌        | 1259/7850 [12:54<32:27,  3.38it/s]
 16%|█▌        | 1260/7850 [12:55<32:14,  3.41it/s]


















100%|██████████| 68/68 [00:37<00:00,  1.92it/s]
























 18%|█▊        | 1417/7850 [14:34<31:35,  3.39it/s]
  0%|          | 0/68 [00:00<?, ?it/s]





















100%|██████████| 68/68 [00:41<00:00,  1.70it/s]

























 20%|██        | 1575/7850 [16:17<30:20,  3.45it/s]
{'loss': 0.5456, 'learning_rate': 0.00013407409151211202, 'epoch': 10.0}


















 99%|█████████▊| 67/68 [00:37<00:00,  1.79it/s]


 20%|██        | 1575/7850 [17:50<1:11:05,  1.47it/s]
{'train_runtime': 1070.6362, 'train_samples_per_second': 234.86, 'train_steps_per_second': 7.332, 'train_loss': 0.5612848530118427, 'epoch': 10.0}