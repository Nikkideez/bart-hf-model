You are adding a <class 'transformers.trainer_callback.DefaultFlowCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
wandb: WARNING Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'learning_rate' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
  0%|          | 0/7850 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.






















  2%|▏         | 157/7850 [00:50<37:34,  3.41it/s]
  0%|          | 0/68 [00:00<?, ?it/s]






















 96%|█████████▌| 65/68 [00:41<00:01,  1.53it/s]























  4%|▍         | 315/7850 [02:37<36:34,  3.43it/s]
  0%|          | 0/68 [00:00<?, ?it/s]





















100%|██████████| 68/68 [00:42<00:00,  1.65it/s]
























  6%|▌         | 470/7850 [04:20<35:52,  3.43it/s]
  6%|▌         | 472/7850 [04:20<36:16,  3.39it/s]






















 99%|█████████▊| 67/68 [00:44<00:00,  1.45it/s]

























  8%|▊         | 630/7850 [06:08<34:59,  3.44it/s]
{'loss': 0.1721, 'learning_rate': 0.00012005201603878226, 'epoch': 4.0}





















 97%|█████████▋| 66/68 [00:43<00:01,  1.52it/s]
























 10%|█         | 786/7850 [07:56<34:39,  3.40it/s]
 10%|█         | 787/7850 [07:56<34:34,  3.40it/s]






















 96%|█████████▌| 65/68 [00:42<00:01,  1.51it/s]


