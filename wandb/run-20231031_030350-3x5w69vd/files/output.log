You are adding a <class 'transformers.trainer_callback.DefaultFlowCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
wandb: WARNING Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'learning_rate' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).














































  2%|▏         | 157/7850 [01:34<1:15:26,  1.70it/s]
  3%|▎         | 2/68 [00:00<00:21,  3.14it/s]





















 94%|█████████▍| 64/68 [00:40<00:02,  1.58it/s]















































  4%|▍         | 313/7850 [04:05<1:16:34,  1.64it/s]
  4%|▍         | 314/7850 [04:06<1:15:50,  1.66it/s]




















 99%|█████████▊| 67/68 [00:41<00:00,  1.60it/s]















































  6%|▌         | 471/7850 [06:36<1:11:56,  1.71it/s]
{'loss': 0.1621, 'learning_rate': 7.385777997086378e-05, 'epoch': 3.0}



















100%|██████████| 68/68 [00:38<00:00,  1.93it/s]
















































  8%|▊         | 628/7850 [09:05<1:10:42,  1.70it/s]
  8%|▊         | 629/7850 [09:06<1:10:47,  1.70it/s]


















100%|██████████| 68/68 [00:37<00:00,  1.94it/s]















































 10%|█         | 785/7850 [12:48<1:09:27,  1.70it/s]
 10%|█         | 786/7850 [12:48<1:09:35,  1.69it/s]



















 99%|█████████▊| 67/68 [00:38<00:00,  1.72it/s]
















































 12%|█▏        | 943/7850 [17:04<1:07:50,  1.70it/s]
{'loss': 0.1525, 'learning_rate': 0.00012172269317086563, 'epoch': 6.0}




















 99%|█████████▊| 67/68 [00:41<00:00,  1.52it/s]
















































 14%|█▍        | 1099/7850 [21:53<1:05:59,  1.71it/s]
 14%|█▍        | 1100/7850 [21:53<1:06:03,  1.70it/s]





















100%|██████████| 68/68 [00:41<00:00,  1.75it/s]

















































 16%|█▌        | 1257/7850 [30:53<1:05:24,  1.68it/s]
 16%|█▌        | 1258/7850 [30:54<1:05:05,  1.69it/s]





















100%|██████████| 68/68 [00:41<00:00,  1.77it/s]

















































 18%|█▊        | 1415/7850 [38:03<1:02:59,  1.70it/s]
{'loss': 0.1385, 'learning_rate': 0.00011341061111405883, 'epoch': 9.0}


















100%|██████████| 68/68 [00:36<00:00,  1.80it/s]















































 20%|██        | 1572/7850 [42:33<1:01:49,  1.69it/s]
  3%|▎         | 2/68 [00:00<00:19,  3.43it/s]





















100%|██████████| 68/68 [00:40<00:00,  1.85it/s]














































 22%|██▏       | 1729/7850 [47:12<59:57,  1.70it/s]
  0%|          | 0/68 [00:00<?, ?it/s]






















 99%|█████████▊| 67/68 [00:41<00:00,  1.58it/s]















































 24%|██▍       | 1887/7850 [51:35<58:28,  1.70it/s]
{'loss': 0.1044, 'learning_rate': 0.00010509852905725203, 'epoch': 12.0}





















 96%|█████████▌| 65/68 [00:41<00:01,  1.58it/s]















































 26%|██▌       | 2043/7850 [56:39<1:00:53,  1.59it/s]
 26%|██▌       | 2044/7850 [56:40<59:41,  1.62it/s]






















100%|██████████| 68/68 [00:42<00:00,  1.72it/s]














































 28%|██▊       | 2201/7850 [1:03:53<55:15,  1.70it/s]
  0%|          | 0/68 [00:00<?, ?it/s]



















 97%|█████████▋| 66/68 [00:35<00:01,  1.79it/s]














































 30%|███       | 2358/7850 [1:08:32<53:12,  1.72it/s]
{'loss': 0.2685, 'learning_rate': 9.680405734378593e-05, 'epoch': 15.0}

















 97%|█████████▋| 66/68 [00:33<00:01,  1.93it/s]
 30%|███       | 2358/7850 [1:09:07<53:12,  1.7Could not locate the best model at /data/ndeo/bart/hypersearch/run_2023-10-30_18-20-08/checkpoint-1258/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.
 30%|███       | 2358/7850 [1:11:24<53:12,  1.72it/s]Traceback (most recent call last):
  File "hyperparameter-search.py", line 134, in train
    trainer.train()
  File "/home/ndeo/.conda/envs/capstone-rnn/lib/python3.8/site-packages/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/ndeo/.conda/envs/capstone-rnn/lib/python3.8/site-packages/transformers/trainer.py", line 1988, in _inner_training_loop
    checkpoints_sorted = self._sorted_checkpoints(use_mtime=False, output_dir=run_dir)
  File "/home/ndeo/.conda/envs/capstone-rnn/lib/python3.8/site-packages/transformers/trainer.py", line 2901, in _sorted_checkpoints
    best_model_index = checkpoints_sorted.index(str(Path(self.state.best_model_checkpoint)))
ValueError: '/data/ndeo/bart/hypersearch/run_2023-10-30_18-20-08/checkpoint-1258' is not in list
{'train_runtime': 4284.3783, 'train_samples_per_second': 58.69, 'train_steps_per_second': 1.832, 'train_loss': 0.23625309412716, 'epoch': 15.0}