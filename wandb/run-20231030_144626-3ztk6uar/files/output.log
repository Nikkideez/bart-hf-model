You are adding a <class 'transformers.trainer_callback.DefaultFlowCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
wandb: WARNING Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'learning_rate' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
  0%|          | 0/7900 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.






















  2%|▏         | 158/7900 [01:08<36:53,  3.50it/s]
{'loss': 3.065, 'learning_rate': 0.00016971972562066965, 'epoch': 1.0}





















100%|██████████| 68/68 [00:42<00:00,  1.65it/s]























  4%|▍         | 313/7900 [03:47<35:09,  3.60it/s]
  4%|▍         | 316/7900 [03:48<33:57,  3.72it/s]





















100%|██████████| 68/68 [00:42<00:00,  1.69it/s]























  6%|▌         | 468/7900 [07:59<34:43,  3.57it/s]

  6%|▌         | 474/7900 [08:01<33:39,  3.68it/s]




















 99%|█████████▊| 67/68 [00:41<00:00,  1.58it/s]
























  8%|▊         | 632/7900 [11:04<33:16,  3.64it/s]
{'loss': 2.4651, 'learning_rate': 0.00015933810864503443, 'epoch': 4.0}




















 96%|█████████▌| 65/68 [00:40<00:01,  1.58it/s]























 10%|▉         | 785/7900 [12:50<36:55,  3.21it/s]
 10%|█         | 790/7900 [12:51<33:41,  3.52it/s]





















 99%|█████████▊| 67/68 [00:41<00:00,  1.58it/s]
























 12%|█▏        | 946/7900 [14:36<33:03,  3.51it/s]
 12%|█▏        | 948/7900 [14:37<31:32,  3.67it/s]





















