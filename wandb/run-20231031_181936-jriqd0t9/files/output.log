You are adding a <class 'transformers.trainer_callback.DefaultFlowCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
wandb: WARNING Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'learning_rate' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
  0%|          | 0/650 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.







  2%|▏         | 13/650 [00:21<13:33,  1.28s/it]
{'loss': 5.7154, 'learning_rate': 7.518417412462777e-06, 'epoch': 0.99}






















 97%|█████████▋| 66/68 [00:45<00:01,  1.44it/s]








  4%|▍         | 25/650 [01:37<16:29,  1.58s/it]

  4%|▍         | 26/650 [01:39<15:12,  1.46s/it]




















100%|██████████| 68/68 [00:41<00:00,  1.79it/s]









  6%|▌         | 38/650 [02:53<16:33,  1.62s/it]
  6%|▌         | 39/650 [02:55<15:17,  1.50s/it]



















 96%|█████████▌| 65/68 [00:37<00:01,  1.65it/s]









  8%|▊         | 51/650 [04:05<16:22,  1.64s/it]

  8%|▊         | 52/650 [04:07<15:15,  1.53s/it]





















 97%|█████████▋| 66/68 [00:43<00:01,  1.52it/s]









 10%|▉         | 64/650 [05:21<15:09,  1.55s/it]
 10%|█         | 65/650 [05:23<14:11,  1.46s/it]





















100%|██████████| 68/68 [00:41<00:00,  1.71it/s]









 12%|█▏        | 78/650 [06:37<14:30,  1.52s/it]
 12%|█▏        | 79/650 [06:39<13:22,  1.40s/it]





















 96%|█████████▌| 65/68 [00:40<00:01,  1.66it/s]








 14%|█▍        | 91/650 [07:53<14:59,  1.61s/it]
 14%|█▍        | 92/650 [07:54<13:49,  1.49s/it]






















 99%|█████████▊| 67/68 [00:44<00:00,  1.46it/s]









 16%|█▌        | 105/650 [09:14<13:39,  1.50s/it]
{'loss': 0.1125, 'learning_rate': 4.5862346216022935e-05, 'epoch': 7.97}






















100%|██████████| 68/68 [00:44<00:00,  1.65it/s]









 18%|█▊        | 117/650 [10:37<14:37,  1.65s/it]
 18%|█▊        | 118/650 [10:39<13:24,  1.51s/it]





















 96%|█████████▌| 65/68 [00:40<00:01,  1.62it/s]








 20%|██        | 131/650 [11:52<12:42,  1.47s/it]
 20%|██        | 131/650 [11:53<12:42,  1.47s/it]




















 94%|█████████▍| 64/68 [00:38<00:02,  1.69it/s]








 22%|██▏       | 143/650 [13:04<13:41,  1.62s/it]

 22%|██▏       | 144/650 [13:06<12:27,  1.48s/it]




















 96%|█████████▌| 65/68 [00:39<00:01,  1.65it/s]









 24%|██▍       | 157/650 [14:21<12:36,  1.54s/it]
 24%|██▍       | 158/650 [14:23<11:52,  1.45s/it]






















 96%|█████████▌| 65/68 [00:43<00:01,  1.51it/s]








 26%|██▋       | 171/650 [15:40<12:09,  1.52s/it]
 26%|██▋       | 171/650 [15:41<12:09,  1.52s/it]






















100%|██████████| 68/68 [00:45<00:00,  1.67it/s]









 28%|██▊       | 183/650 [16:59<12:54,  1.66s/it]
 28%|██▊       | 184/650 [17:01<12:04,  1.55s/it]






















100%|██████████| 68/68 [00:44<00:00,  1.70it/s]









 30%|███       | 197/650 [18:18<11:40,  1.55s/it]
{'loss': 0.0873, 'learning_rate': 3.8176852861060984e-05, 'epoch': 14.96}





















 99%|█████████▊| 67/68 [00:43<00:00,  1.56it/s]


