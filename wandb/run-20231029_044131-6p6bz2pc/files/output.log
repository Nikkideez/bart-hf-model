  0%|                                                                                                                                                                                                                                         | 0/316 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  4%|█████████▉                                                                                                                                                                                                                      | 14/316 [00:01<00:35,  8.59it/s]
{'loss': 8.1716, 'learning_rate': 1.624749612285061e-05, 'epoch': 0.01}
{'loss': 7.5378, 'learning_rate': 1.624749612285061e-05, 'epoch': 0.01}
{'loss': 7.858, 'learning_rate': 1.624749612285061e-05, 'epoch': 0.02}
{'loss': 7.8554, 'learning_rate': 1.6196079995879565e-05, 'epoch': 0.03}
{'loss': 6.5296, 'learning_rate': 1.614466386890852e-05, 'epoch': 0.03}
{'loss': 5.6373, 'learning_rate': 1.6093247741937472e-05, 'epoch': 0.04}
{'loss': 4.6143, 'learning_rate': 1.604183161496643e-05, 'epoch': 0.04}
{'loss': 4.3592, 'learning_rate': 1.5990415487995383e-05, 'epoch': 0.05}
{'loss': 3.7077, 'learning_rate': 1.5938999361024336e-05, 'epoch': 0.06}
{'loss': 3.4801, 'learning_rate': 1.588758323405329e-05, 'epoch': 0.06}
{'loss': 3.3426, 'learning_rate': 1.5836167107082243e-05, 'epoch': 0.07}
{'loss': 3.4016, 'learning_rate': 1.5784750980111196e-05, 'epoch': 0.08}
{'loss': 2.6999, 'learning_rate': 1.5784750980111196e-05, 'epoch': 0.08}

 10%|█████████████████████▉                                                                                                                                                                                                          | 31/316 [00:03<00:35,  7.99it/s]
{'loss': 2.8225, 'learning_rate': 1.5681918726169103e-05, 'epoch': 0.09}
{'loss': 2.4811, 'learning_rate': 1.5630502599198057e-05, 'epoch': 0.1}
{'loss': 2.3089, 'learning_rate': 1.557908647222701e-05, 'epoch': 0.11}
{'loss': 2.2454, 'learning_rate': 1.5527670345255964e-05, 'epoch': 0.11}
{'loss': 2.3891, 'learning_rate': 1.5476254218284917e-05, 'epoch': 0.12}
{'loss': 1.9476, 'learning_rate': 1.542483809131387e-05, 'epoch': 0.13}
{'loss': 1.8681, 'learning_rate': 1.5373421964342828e-05, 'epoch': 0.13}
{'loss': 1.8566, 'learning_rate': 1.532200583737178e-05, 'epoch': 0.14}
{'loss': 1.7786, 'learning_rate': 1.5270589710400735e-05, 'epoch': 0.15}
{'loss': 1.6212, 'learning_rate': 1.5219173583429687e-05, 'epoch': 0.15}
{'loss': 1.5291, 'learning_rate': 1.516775745645864e-05, 'epoch': 0.16}
{'loss': 1.4593, 'learning_rate': 1.5116341329487594e-05, 'epoch': 0.16}
{'loss': 1.4984, 'learning_rate': 1.5064925202516547e-05, 'epoch': 0.17}
{'loss': 1.5381, 'learning_rate': 1.5013509075545502e-05, 'epoch': 0.18}
{'loss': 1.4934, 'learning_rate': 1.4962092948574456e-05, 'epoch': 0.18}
{'loss': 1.3233, 'learning_rate': 1.491067682160341e-05, 'epoch': 0.19}

 15%|██████████████████████████████████                                                                                                                                                                                              | 48/316 [00:05<00:30,  8.72it/s]
{'loss': 1.0258, 'learning_rate': 1.4807844567661316e-05, 'epoch': 0.2}
{'loss': 1.1101, 'learning_rate': 1.475642844069027e-05, 'epoch': 0.21}
{'loss': 1.2004, 'learning_rate': 1.4705012313719225e-05, 'epoch': 0.22}
{'loss': 1.0943, 'learning_rate': 1.4653596186748178e-05, 'epoch': 0.22}
{'loss': 1.2876, 'learning_rate': 1.4602180059777132e-05, 'epoch': 0.23}
{'loss': 1.0859, 'learning_rate': 1.4550763932806085e-05, 'epoch': 0.23}
{'loss': 1.0793, 'learning_rate': 1.4499347805835039e-05, 'epoch': 0.24}
{'loss': 1.2904, 'learning_rate': 1.4447931678863992e-05, 'epoch': 0.25}
{'loss': 1.0278, 'learning_rate': 1.4396515551892946e-05, 'epoch': 0.25}
{'loss': 1.3091, 'learning_rate': 1.4345099424921901e-05, 'epoch': 0.26}
{'loss': 1.0267, 'learning_rate': 1.4293683297950855e-05, 'epoch': 0.27}
{'loss': 0.9458, 'learning_rate': 1.4242267170979808e-05, 'epoch': 0.27}
{'loss': 0.7434, 'learning_rate': 1.4190851044008762e-05, 'epoch': 0.28}
{'loss': 1.1966, 'learning_rate': 1.4139434917037715e-05, 'epoch': 0.28}
{'loss': 0.8449, 'learning_rate': 1.4088018790066669e-05, 'epoch': 0.29}

 20%|█████████████████████████████████████████████▎                                                                                                                                                                                  | 64/316 [00:07<00:32,  7.65it/s]
{'loss': 0.8731, 'learning_rate': 1.3985186536124577e-05, 'epoch': 0.3}
{'loss': 0.9402, 'learning_rate': 1.393377040915353e-05, 'epoch': 0.31}
{'loss': 1.1183, 'learning_rate': 1.3882354282182484e-05, 'epoch': 0.32}
{'loss': 0.7607, 'learning_rate': 1.3830938155211438e-05, 'epoch': 0.32}
{'loss': 1.0654, 'learning_rate': 1.3779522028240391e-05, 'epoch': 0.33}
{'loss': 0.9423, 'learning_rate': 1.3728105901269345e-05, 'epoch': 0.34}
{'loss': 1.1034, 'learning_rate': 1.36766897742983e-05, 'epoch': 0.34}
{'loss': 0.662, 'learning_rate': 1.3625273647327253e-05, 'epoch': 0.35}
{'loss': 0.8035, 'learning_rate': 1.3573857520356207e-05, 'epoch': 0.35}
{'loss': 1.007, 'learning_rate': 1.352244139338516e-05, 'epoch': 0.36}
{'loss': 0.7519, 'learning_rate': 1.3471025266414114e-05, 'epoch': 0.37}
{'loss': 0.8674, 'learning_rate': 1.3419609139443067e-05, 'epoch': 0.37}
{'loss': 0.6865, 'learning_rate': 1.3368193012472022e-05, 'epoch': 0.38}
{'loss': 1.0297, 'learning_rate': 1.3316776885500976e-05, 'epoch': 0.39}
{'loss': 0.8959, 'learning_rate': 1.326536075852993e-05, 'epoch': 0.39}

 25%|████████████████████████████████████████████████████████▋                                                                                                                                                                       | 80/316 [00:09<00:28,  8.15it/s]
{'loss': 0.672, 'learning_rate': 1.3162528504587836e-05, 'epoch': 0.41}
{'loss': 0.7458, 'learning_rate': 1.311111237761679e-05, 'epoch': 0.41}
{'loss': 0.6273, 'learning_rate': 1.3059696250645743e-05, 'epoch': 0.42}
{'loss': 0.6553, 'learning_rate': 1.3008280123674699e-05, 'epoch': 0.42}
{'loss': 0.6523, 'learning_rate': 1.2956863996703652e-05, 'epoch': 0.43}
{'loss': 0.7428, 'learning_rate': 1.2905447869732606e-05, 'epoch': 0.44}
{'loss': 0.9323, 'learning_rate': 1.2854031742761559e-05, 'epoch': 0.44}
{'loss': 0.5378, 'learning_rate': 1.2802615615790513e-05, 'epoch': 0.45}
{'loss': 0.7018, 'learning_rate': 1.2751199488819466e-05, 'epoch': 0.46}
{'loss': 0.7513, 'learning_rate': 1.269978336184842e-05, 'epoch': 0.46}
{'loss': 0.7455, 'learning_rate': 1.2648367234877375e-05, 'epoch': 0.47}
{'loss': 0.707, 'learning_rate': 1.2596951107906328e-05, 'epoch': 0.47}
{'loss': 0.5562, 'learning_rate': 1.2545534980935282e-05, 'epoch': 0.48}
{'loss': 0.7974, 'learning_rate': 1.2494118853964235e-05, 'epoch': 0.49}
{'loss': 0.7569, 'learning_rate': 1.2442702726993189e-05, 'epoch': 0.49}
{'loss': 0.52, 'learning_rate': 1.2391286600022142e-05, 'epoch': 0.5}

 31%|████████████████████████████████████████████████████████████████████▊                                                                                                                                                           | 97/316 [00:12<00:26,  8.25it/s]
{'loss': 0.7476, 'learning_rate': 1.2288454346080053e-05, 'epoch': 0.51}
{'loss': 0.5581, 'learning_rate': 1.2237038219109006e-05, 'epoch': 0.52}
{'loss': 0.6454, 'learning_rate': 1.2185622092137958e-05, 'epoch': 0.53}
{'loss': 0.6168, 'learning_rate': 1.2134205965166911e-05, 'epoch': 0.53}
{'loss': 0.704, 'learning_rate': 1.2082789838195865e-05, 'epoch': 0.54}
{'loss': 0.6387, 'learning_rate': 1.2031373711224818e-05, 'epoch': 0.54}
{'loss': 0.5095, 'learning_rate': 1.1979957584253775e-05, 'epoch': 0.55}
{'loss': 0.6796, 'learning_rate': 1.1928541457282729e-05, 'epoch': 0.56}
{'loss': 0.4611, 'learning_rate': 1.1877125330311682e-05, 'epoch': 0.56}
{'loss': 0.7667, 'learning_rate': 1.1825709203340636e-05, 'epoch': 0.57}
{'loss': 0.5902, 'learning_rate': 1.177429307636959e-05, 'epoch': 0.58}
{'loss': 0.6766, 'learning_rate': 1.1722876949398543e-05, 'epoch': 0.58}
{'loss': 0.7326, 'learning_rate': 1.1671460822427498e-05, 'epoch': 0.59}
{'loss': 0.5199, 'learning_rate': 1.1620044695456451e-05, 'epoch': 0.59}

 36%|███████████████████████████████████████████████████████████████████████████████▋                                                                                                                                               | 113/316 [00:14<00:24,  8.15it/s]
{'loss': 0.626, 'learning_rate': 1.1517212441514358e-05, 'epoch': 0.61}
{'loss': 0.6688, 'learning_rate': 1.1465796314543312e-05, 'epoch': 0.61}
{'loss': 0.4962, 'learning_rate': 1.1414380187572265e-05, 'epoch': 0.62}
{'loss': 0.5576, 'learning_rate': 1.1362964060601219e-05, 'epoch': 0.63}
{'loss': 0.5938, 'learning_rate': 1.1311547933630174e-05, 'epoch': 0.63}
{'loss': 0.7562, 'learning_rate': 1.1260131806659128e-05, 'epoch': 0.64}
{'loss': 0.5312, 'learning_rate': 1.1208715679688081e-05, 'epoch': 0.65}
{'loss': 0.6886, 'learning_rate': 1.1157299552717035e-05, 'epoch': 0.65}
{'loss': 0.5512, 'learning_rate': 1.1105883425745988e-05, 'epoch': 0.66}
{'loss': 0.5594, 'learning_rate': 1.1105883425745988e-05, 'epoch': 0.66}
{'loss': 0.8598, 'learning_rate': 1.1054467298774942e-05, 'epoch': 0.67}
{'loss': 0.6854, 'learning_rate': 1.1003051171803895e-05, 'epoch': 0.68}
{'loss': 0.6243, 'learning_rate': 1.095163504483285e-05, 'epoch': 0.68}
{'loss': 0.7974, 'learning_rate': 1.0900218917861804e-05, 'epoch': 0.69}
{'loss': 0.6396, 'learning_rate': 1.0848802790890757e-05, 'epoch': 0.7}
{'loss': 0.5382, 'learning_rate': 1.079738666391971e-05, 'epoch': 0.7}

 41%|████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                  | 131/316 [00:16<00:21,  8.42it/s]
{'loss': 0.5921, 'learning_rate': 1.0694554409977618e-05, 'epoch': 0.72}
{'loss': 0.5925, 'learning_rate': 1.0643138283006573e-05, 'epoch': 0.72}
{'loss': 0.5455, 'learning_rate': 1.0591722156035526e-05, 'epoch': 0.73}
{'loss': 0.5145, 'learning_rate': 1.054030602906448e-05, 'epoch': 0.73}
{'loss': 0.4498, 'learning_rate': 1.0488889902093433e-05, 'epoch': 0.74}
{'loss': 0.5432, 'learning_rate': 1.0437473775122387e-05, 'epoch': 0.75}
{'loss': 0.5372, 'learning_rate': 1.038605764815134e-05, 'epoch': 0.75}
{'loss': 0.6026, 'learning_rate': 1.0334641521180294e-05, 'epoch': 0.76}
{'loss': 0.7044, 'learning_rate': 1.0283225394209249e-05, 'epoch': 0.77}
{'loss': 0.4689, 'learning_rate': 1.0231809267238202e-05, 'epoch': 0.77}
{'loss': 0.6649, 'learning_rate': 1.0180393140267156e-05, 'epoch': 0.78}
{'loss': 0.4159, 'learning_rate': 1.012897701329611e-05, 'epoch': 0.78}
{'loss': 0.5149, 'learning_rate': 1.0077560886325063e-05, 'epoch': 0.79}
{'loss': 0.5433, 'learning_rate': 1.0026144759354016e-05, 'epoch': 0.8}
{'loss': 0.5298, 'learning_rate': 9.974728632382972e-06, 'epoch': 0.8}

 47%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                       | 147/316 [00:18<00:21,  7.75it/s]
{'loss': 0.4368, 'learning_rate': 9.871896378440879e-06, 'epoch': 0.82}
{'loss': 0.4294, 'learning_rate': 9.820480251469832e-06, 'epoch': 0.82}
{'loss': 0.5278, 'learning_rate': 9.769064124498786e-06, 'epoch': 0.83}
{'loss': 0.5049, 'learning_rate': 9.717647997527739e-06, 'epoch': 0.84}
{'loss': 0.3545, 'learning_rate': 9.666231870556693e-06, 'epoch': 0.84}
{'loss': 0.5371, 'learning_rate': 9.614815743585648e-06, 'epoch': 0.85}
{'loss': 0.4472, 'learning_rate': 9.563399616614601e-06, 'epoch': 0.85}
{'loss': 0.6097, 'learning_rate': 9.511983489643555e-06, 'epoch': 0.86}
{'loss': 0.511, 'learning_rate': 9.460567362672508e-06, 'epoch': 0.87}
{'loss': 0.3981, 'learning_rate': 9.409151235701462e-06, 'epoch': 0.87}
{'loss': 0.4393, 'learning_rate': 9.357735108730415e-06, 'epoch': 0.88}
{'loss': 0.5546, 'learning_rate': 9.30631898175937e-06, 'epoch': 0.89}
{'loss': 0.3989, 'learning_rate': 9.254902854788324e-06, 'epoch': 0.89}
{'loss': 0.5142, 'learning_rate': 9.203486727817277e-06, 'epoch': 0.9}
{'loss': 0.4226, 'learning_rate': 9.152070600846231e-06, 'epoch': 0.91}
{'loss': 0.4836, 'learning_rate': 9.100654473875184e-06, 'epoch': 0.91}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                               | 158/316 [00:19<00:18,  8.41it/s]
  0%|                                                                                                                                                                                                                                          | 0/34 [00:00<?, ?it/s]
{'loss': 0.6076, 'learning_rate': 8.997822219933091e-06, 'epoch': 0.92}
{'loss': 0.3861, 'learning_rate': 8.946406092962047e-06, 'epoch': 0.93}
{'loss': 0.4644, 'learning_rate': 8.894989965991e-06, 'epoch': 0.94}
{'loss': 0.4323, 'learning_rate': 8.843573839019954e-06, 'epoch': 0.94}
{'loss': 0.5958, 'learning_rate': 8.792157712048907e-06, 'epoch': 0.95}
{'loss': 0.5168, 'learning_rate': 8.74074158507786e-06, 'epoch': 0.96}
{'loss': 0.497, 'learning_rate': 8.689325458106814e-06, 'epoch': 0.96}
{'loss': 0.5255, 'learning_rate': 8.637909331135768e-06, 'epoch': 0.97}
{'loss': 0.5144, 'learning_rate': 8.586493204164723e-06, 'epoch': 0.97}
{'loss': 0.3801, 'learning_rate': 8.535077077193676e-06, 'epoch': 0.98}
{'loss': 0.4592, 'learning_rate': 8.48366095022263e-06, 'epoch': 0.99}
{'loss': 0.4557, 'learning_rate': 8.432244823251583e-06, 'epoch': 0.99}






  File "/home/nikx/Documents/Projects/capstone/bart/model/train-model.py", line 67, in <module>███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 34/34 [00:13<00:00,  2.69it/s]
    trainer.train()
  File "/home/nikx/capstone/bart-env/lib/python3.9/site-packages/transformers/trainer.py", line 1553, in train
    return inner_training_loop(
  File "/home/nikx/capstone/bart-env/lib/python3.9/site-packages/transformers/trainer.py", line 1942, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/nikx/capstone/bart-env/lib/python3.9/site-packages/transformers/trainer.py", line 2254, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/nikx/capstone/bart-env/lib/python3.9/site-packages/transformers/trainer_seq2seq.py", line 159, in evaluate
    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
  File "/home/nikx/capstone/bart-env/lib/python3.9/site-packages/transformers/trainer.py", line 2968, in evaluate
    output = eval_loop(
  File "/home/nikx/capstone/bart-env/lib/python3.9/site-packages/transformers/trainer.py", line 3261, in evaluation_loop
    metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))
  File "/home/nikx/Documents/Projects/capstone/bart/model/metrics.py", line 62, in compute_metrics
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
NameError: name 'tokenizer' is not defined