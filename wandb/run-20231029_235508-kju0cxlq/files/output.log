You are adding a <class 'transformers.trainer_callback.DefaultFlowCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
wandb: WARNING Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'learning_rate' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
  0%|          | 0/1975 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.






  4%|▍         | 79/1975 [00:15<05:34,  5.66it/s]
  3%|▎         | 2/68 [00:00<00:11,  5.99it/s]













 97%|█████████▋| 66/68 [00:24<00:00,  2.78it/s]








  8%|▊         | 157/1975 [04:26<05:44,  5.27it/s]
  8%|▊         | 158/1975 [04:26<05:31,  5.48it/s]












 96%|█████████▌| 65/68 [00:23<00:01,  2.74it/s]








 12%|█▏        | 235/1975 [05:54<05:16,  5.49it/s]
 12%|█▏        | 237/1975 [05:54<05:15,  5.51it/s]














 96%|█████████▌| 65/68 [00:27<00:01,  2.42it/s]








 16%|█▌        | 314/1975 [07:14<05:11,  5.34it/s]
 16%|█▌        | 316/1975 [07:14<05:01,  5.51it/s]












100%|██████████| 68/68 [00:25<00:00,  3.10it/s]









 20%|██        | 395/1975 [08:18<04:47,  5.50it/s]
{'loss': 0.0831, 'learning_rate': 9.255901395898202e-05, 'epoch': 5.0}












100%|██████████| 68/68 [00:25<00:00,  2.92it/s]








 24%|██▎       | 467/1975 [09:14<04:32,  5.54it/s]
 24%|██▍       | 474/1975 [09:16<04:30,  5.54it/s]












 96%|█████████▌| 65/68 [00:24<00:01,  2.84it/s]








 28%|██▊       | 553/1975 [10:06<04:17,  5.53it/s]
  0%|          | 0/68 [00:00<?, ?it/s]













 96%|█████████▌| 65/68 [00:25<00:01,  2.56it/s]








 32%|███▏      | 628/1975 [10:56<04:05,  5.49it/s]
 32%|███▏      | 632/1975 [10:57<04:02,  5.53it/s]













 99%|█████████▊| 67/68 [00:26<00:00,  2.46it/s]








 36%|███▌      | 711/1975 [11:46<03:52,  5.45it/s]
  0%|          | 0/68 [00:00<?, ?it/s]













 99%|█████████▊| 67/68 [00:26<00:00,  2.50it/s]








 40%|███▉      | 787/1975 [12:35<03:37,  5.46it/s]
 40%|████      | 790/1975 [12:35<03:31,  5.61it/s]













 94%|█████████▍| 64/68 [00:24<00:01,  2.49it/s]








 44%|████▍     | 869/1975 [13:23<03:23,  5.44it/s]
{'loss': 0.064, 'learning_rate': 6.486143023640785e-05, 'epoch': 11.0}












 94%|█████████▍| 64/68 [00:23<00:01,  2.77it/s]








 48%|████▊     | 947/1975 [14:11<03:08,  5.44it/s]
 48%|████▊     | 948/1975 [14:11<03:03,  5.59it/s]












 96%|█████████▌| 65/68 [00:25<00:01,  2.65it/s]








 52%|█████▏    | 1027/1975 [15:02<02:46,  5.69it/s]
  0%|          | 0/68 [00:00<?, ?it/s]













 99%|█████████▊| 67/68 [00:26<00:00,  2.48it/s]








 56%|█████▌    | 1106/1975 [15:53<02:35,  5.59it/s]
  0%|          | 0/68 [00:00<?, ?it/s]













 93%|█████████▎| 63/68 [00:24<00:01,  2.61it/s]








 60%|█████▉    | 1184/1975 [16:43<02:29,  5.29it/s]
 60%|██████    | 1185/1975 [16:43<02:26,  5.41it/s]













 94%|█████████▍| 64/68 [00:25<00:01,  2.48it/s]








 64%|██████▍   | 1264/1975 [17:31<02:08,  5.55it/s]
{'loss': 0.0527, 'learning_rate': 4.1780110467596045e-05, 'epoch': 16.0}




