You are adding a <class 'transformers.trainer_callback.DefaultFlowCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
wandb: WARNING Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'learning_rate' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
  0%|          | 0/1950 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.




  2%|▏         | 39/1950 [00:09<06:51,  4.65it/s]
  4%|▍         | 3/68 [00:00<00:15,  4.11it/s]












100%|██████████| 68/68 [00:24<00:00,  2.85it/s]






  4%|▍         | 79/1950 [01:30<06:23,  4.88it/s]
{'loss': 0.4214, 'learning_rate': 3.1638451145274266e-05, 'epoch': 2.0}













 97%|█████████▋| 66/68 [00:25<00:00,  2.64it/s]




  6%|▌         | 118/1950 [02:28<07:02,  4.33it/s]
  0%|          | 0/68 [00:00<?, ?it/s]













 99%|█████████▊| 67/68 [00:24<00:00,  2.80it/s]




  8%|▊         | 158/1950 [04:38<06:18,  4.74it/s]
  3%|▎         | 2/68 [00:00<00:10,  6.03it/s]













 94%|█████████▍| 64/68 [00:24<00:01,  2.68it/s]




 10%|█         | 197/1950 [05:25<06:20,  4.60it/s]
  4%|▍         | 3/68 [00:00<00:16,  4.05it/s]












 94%|█████████▍| 64/68 [00:22<00:01,  2.79it/s]





 12%|█▏        | 235/1950 [06:18<06:18,  4.53it/s]
 12%|█▏        | 237/1950 [06:19<05:59,  4.76it/s]













100%|██████████| 68/68 [00:26<00:00,  2.81it/s]






 14%|█▍        | 274/1950 [07:32<06:09,  4.54it/s]
 14%|█▍        | 276/1950 [07:33<06:15,  4.45it/s]












 94%|█████████▍| 64/68 [00:23<00:01,  2.80it/s]





 16%|█▌        | 314/1950 [08:22<06:02,  4.51it/s]
 16%|█▌        | 316/1950 [08:23<05:35,  4.87it/s]














100%|██████████| 68/68 [00:27<00:00,  2.68it/s]




 18%|█▊        | 352/1950 [09:43<05:59,  4.45it/s]
 18%|█▊        | 355/1950 [09:43<06:05,  4.37it/s]













 99%|█████████▊| 67/68 [00:24<00:00,  2.74it/s]




 20%|██        | 395/1950 [10:44<05:31,  4.69it/s]
  0%|          | 0/68 [00:00<?, ?it/s]














100%|██████████| 68/68 [00:25<00:00,  3.03it/s]




 22%|██▏       | 434/1950 [12:12<05:35,  4.52it/s]
  4%|▍         | 3/68 [00:00<00:15,  4.22it/s]












 96%|█████████▌| 65/68 [00:22<00:01,  2.75it/s]




 24%|██▍       | 474/1950 [13:42<05:13,  4.70it/s]
  0%|          | 0/68 [00:00<?, ?it/s]













100%|██████████| 68/68 [00:24<00:00,  3.04it/s]






 26%|██▌       | 511/1950 [14:59<05:25,  4.42it/s]
 26%|██▋       | 513/1950 [14:59<05:16,  4.55it/s]













 97%|█████████▋| 66/68 [00:25<00:00,  2.76it/s]




 28%|██▊       | 553/1950 [16:30<04:56,  4.71it/s]
  3%|▎         | 2/68 [00:00<00:13,  4.94it/s]













100%|██████████| 68/68 [00:25<00:00,  3.10it/s]






 30%|███       | 592/1950 [17:45<04:57,  4.56it/s]
{'loss': 0.0641, 'learning_rate': 6.21355440241245e-05, 'epoch': 14.99}












100%|██████████| 68/68 [00:25<00:00,  3.01it/s]





 32%|███▏      | 632/1950 [20:29<04:39,  4.71it/s]
  0%|          | 0/68 [00:00<?, ?it/s]













100%|██████████| 68/68 [00:26<00:00,  2.87it/s]


 32%|███▏      | 632/1950 [21:43<45:17,  2.06s/it]
{'train_runtime': 1303.0319, 'train_samples_per_second': 192.973, 'train_steps_per_second': 1.497, 'train_loss': 0.3467152744908876, 'epoch': 16.0}