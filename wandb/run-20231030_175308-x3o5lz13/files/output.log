Traceback (most recent call last):
  File "hyperparameter-search.py", line 114, in train
    trainer = Seq2SeqTrainer(
  File "/home/ndeo/.conda/envs/capstone-rnn/lib/python3.8/site-packages/transformers/trainer_seq2seq.py", line 56, in __init__
    super().__init__(
  File "/home/ndeo/.conda/envs/capstone-rnn/lib/python3.8/site-packages/transformers/trainer.py", line 506, in __init__
    self._move_model_to_device(model, args.device)
  File "/home/ndeo/.conda/envs/capstone-rnn/lib/python3.8/site-packages/transformers/trainer.py", line 730, in _move_model_to_device
    model = model.to(device)
  File "/home/ndeo/.conda/envs/capstone-rnn/lib/python3.8/site-packages/transformers/modeling_utils.py", line 2053, in to
    return super().to(*args, **kwargs)
  File "/home/ndeo/.conda/envs/capstone-rnn/lib/python3.8/site-packages/torch/nn/modules/module.py", line 927, in to
    return self._apply(convert)
  File "/home/ndeo/.conda/envs/capstone-rnn/lib/python3.8/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  File "/home/ndeo/.conda/envs/capstone-rnn/lib/python3.8/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  File "/home/ndeo/.conda/envs/capstone-rnn/lib/python3.8/site-packages/torch/nn/modules/module.py", line 602, in _apply
    param_applied = fn(param)
  File "/home/ndeo/.conda/envs/capstone-rnn/lib/python3.8/site-packages/torch/nn/modules/module.py", line 925, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 198.00 MiB (GPU 0; 15.74 GiB total capacity; 14.66 GiB already allocated; 6.00 MiB free; 14.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF