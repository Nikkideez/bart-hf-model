You are adding a <class 'transformers.trainer_callback.DefaultFlowCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
wandb: WARNING Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'learning_rate' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
  0%|          | 0/1300 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.











  2%|▏         | 26/1300 [00:26<21:05,  1.01it/s]
  0%|          | 0/68 [00:00<?, ?it/s]



















 97%|█████████▋| 66/68 [00:35<00:01,  1.83it/s]













  4%|▍         | 51/1300 [03:16<20:52,  1.00s/it]
  4%|▍         | 52/1300 [03:17<20:48,  1.00s/it]





















100%|██████████| 68/68 [00:41<00:00,  1.81it/s]














  6%|▌         | 78/1300 [05:24<20:17,  1.00it/s]
  6%|▌         | 79/1300 [05:25<20:04,  1.01it/s]




















 99%|█████████▊| 67/68 [00:40<00:00,  1.50it/s]














  8%|▊         | 105/1300 [07:18<20:05,  1.01s/it]
{'loss': 0.4806, 'learning_rate': 0.0004260846891523072, 'epoch': 3.99}


















100%|██████████| 68/68 [00:36<00:00,  1.91it/s]













 10%|█         | 130/1300 [09:52<19:30,  1.00s/it]
 10%|█         | 131/1300 [09:54<19:25,  1.00it/s]



















 97%|█████████▋| 66/68 [00:38<00:01,  1.67it/s]














 12%|█▏        | 157/1300 [12:44<19:08,  1.00s/it]
 12%|█▏        | 158/1300 [12:45<18:42,  1.02it/s]




















 96%|█████████▌| 65/68 [00:38<00:01,  1.68it/s]













 14%|█▍        | 183/1300 [15:28<18:19,  1.02it/s]
 14%|█▍        | 184/1300 [15:29<18:17,  1.02it/s]



















 96%|█████████▌| 65/68 [00:38<00:01,  1.71it/s]














 16%|█▌        | 210/1300 [17:54<18:06,  1.00it/s]
 16%|█▌        | 210/1300 [17:55<18:06,  1.00it/s]

















 96%|█████████▌| 65/68 [00:35<00:01,  1.86it/s]














 18%|█▊        | 235/1300 [20:31<17:45,  1.00s/it]

 18%|█▊        | 237/1300 [20:33<17:18,  1.02it/s]




















 97%|█████████▋| 66/68 [00:41<00:01,  1.57it/s]














 20%|██        | 263/1300 [22:51<17:14,  1.00it/s]
{'loss': 0.5893, 'learning_rate': 0.00036993674846334595, 'epoch': 9.99}
















 99%|█████████▊| 67/68 [00:50<00:01,  1.10s/it]














 22%|██▏       | 289/1300 [27:06<16:54,  1.00s/it]
 22%|██▏       | 289/1300 [27:07<16:54,  1.00s/it]



















 99%|█████████▊| 67/68 [00:38<00:00,  1.76it/s]














 24%|██▍       | 316/1300 [29:24<15:50,  1.04it/s]
{'loss': 0.7283, 'learning_rate': 0.00035110231266261845, 'epoch': 12.0}

















 99%|█████████▊| 67/68 [00:33<00:00,  1.97it/s]

 24%|██▍       | 316/1300 [34:12<1:46:32,  6.50s/it]
{'train_runtime': 2052.7611, 'train_samples_per_second': 122.494, 'train_steps_per_second': 0.633, 'train_loss': 0.659719934946374, 'epoch': 12.0}