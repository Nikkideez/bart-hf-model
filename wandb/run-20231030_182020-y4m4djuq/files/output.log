You are adding a <class 'transformers.trainer_callback.DefaultFlowCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
WandbCallback
wandb: WARNING Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'learning_rate' was locked by 'sweep' (ignored update).
wandb: WARNING Config item 'weight_decay' was locked by 'sweep' (ignored update).
  0%|          | 0/31450 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
















































































  2%|▏         | 627/31450 [02:42<2:12:02,  3.89it/s]
  2%|▏         | 629/31450 [02:42<2:11:43,  3.90it/s]



















 96%|█████████▌| 65/68 [00:38<00:01,  1.70it/s]

















































































  4%|▍         | 1250/31450 [08:44<2:07:57,  3.93it/s]

  4%|▍         | 1258/31450 [08:46<2:08:25,  3.92it/s]


















 99%|█████████▊| 67/68 [00:37<00:00,  1.79it/s]

















































































  6%|▌         | 1881/31450 [13:29<2:04:46,  3.95it/s]
  6%|▌         | 1887/31450 [13:30<2:05:46,  3.92it/s]




















 97%|█████████▋| 66/68 [00:39<00:01,  1.70it/s]

















































































  8%|▊         | 2509/31450 [19:57<2:02:42,  3.93it/s]

  8%|▊         | 2516/31450 [19:59<2:02:52,  3.92it/s]
















 96%|█████████▌| 65/68 [00:33<00:01,  1.92it/s]


















































































 10%|▉         | 3144/31450 [24:22<2:00:44,  3.91it/s]
 10%|█         | 3145/31450 [24:22<2:00:34,  3.91it/s]


















100%|██████████| 68/68 [00:36<00:00,  1.81it/s]



















































































 12%|█▏        | 3773/31450 [28:40<2:06:00,  3.66it/s]
 12%|█▏        | 3774/31450 [28:40<2:06:01,  3.66it/s]




















 97%|█████████▋| 66/68 [00:41<00:01,  1.70it/s]


















































































 14%|█▍        | 4402/31450 [33:04<1:53:56,  3.96it/s]
 14%|█▍        | 4403/31450 [33:05<1:54:12,  3.95it/s]















100%|██████████| 68/68 [00:31<00:00,  2.30it/s]

















































































 16%|█▌        | 5025/31450 [37:07<1:59:36,  3.68it/s]

 16%|█▌        | 5032/31450 [37:08<1:52:49,  3.90it/s]





















 99%|█████████▊| 67/68 [00:43<00:00,  1.52it/s]



















































































 18%|█▊        | 5661/31450 [41:24<1:49:20,  3.93it/s]
{'loss': 1.497, 'learning_rate': 0.00016228296004058447, 'epoch': 9.0}






















100%|██████████| 68/68 [00:43<00:00,  1.66it/s]
















































































 20%|██        | 6290/31450 [47:27<1:46:07,  3.95it/s]
  3%|▎         | 2/68 [00:00<00:21,  3.03it/s]






















 99%|█████████▊| 67/68 [00:42<00:00,  1.55it/s]
{'eval_loss': 1.3000818490982056, 'eval_bleu': 14.0946, 'eval_exact_match': 0.0, 'eval_spot_acc': 0.0, 'eval_gen_len': 20.0, 'eval_runtime': 44.8871, 'eval_samples_per_second': 24.016, 'eval_steps_per_second': 1.515, 'epoch': 10.0}

 20%|██        | 6290/31450 [50:33<3:22:13,  2.07it/s]